## About

I am a Ph.D. student of Mathematical Sciences at Seoul National University, advised by Professor [Ernest K. Ryu](https://ernestryu.com). My current interest is in generative models, especially diffusion probabilistic models and large language models.

---

## Publications 

[Simple Drop-in LoRA Conditioning on Attention Layers Will Improve Your Diffusion Model](https://arxiv.org/abs/2405.03958)
Joo Young Choi, Jaesung R. Park, Inkyu Park, Jaewoong Cho, Albert No, Ernest K. Ryu *Manuscript*

[LoRA can Replace Time and Class Embeddings in Diffusion Probabilistic Models](pdf/lora_diffusion.pdf)
Joo Young Choi, Jaesung Park, Inkyu Park, Jaewoong Cho, Albert No, Ernest K. Ryu. *NeurIPS 2023 Workshop on Diffusion Models*

[Diffusion Probabilistic Models Generalize when They Fail to Memorize](https://openreview.net/forum?id=shciCbSk9h)
TaeHo Yoon, Joo Young Choi, Sehyun Kwon, Ernest K. Ryu. *ICML 2023 Workshop SPIGM*

[Rotation and Translation Invariant Representation Learning with Implicit Neural Representations](http://proceedings.mlr.press/v202/kwon23a/kwon23a.pdf)
Sehyun Kwon, Joo Young Choi, Ernest K. Ryu. *ICML 2023*

[Neural Tangent Kernel Analysis of Deep Narrow Neural Networks](https://proceedings.mlr.press/v162/lee22a.html)
Jongmin Lee, Joo Young Choi, Ernest K. Ryu, Albert No. *ICML 2022*

---
## Education

**Seoul National University** (September 2018 - )
<br>
Ph.D in Mathematical Sciences

**Korea University** (March 2012 - August 2018)
<br>
B.S in Business Administration
<br>
B.S in Mathematics

---

## Experience

**Teaching Assistant** (Seoul National University)

- Topics in Machine Intelligence: Generative AI and Foundation Models, M3309.001800, Spring 2024.
- Mathematical Foundations of Deep Neural Networks, M1407.001200, Fall 2022. [Outstanding TA Award](http://www.math.snu.ac.kr/board/index.php?mid=page_iFgL02)
- Topics in Applied Mathematics: Infinitely Large Neural Networks, 3341.751, Spring 2022.
- Mathematical Foundations of Deep Neural Networks, M1407.001200, Fall 2021.

---

## Notes

Study of papers that are of personal interest

- *T-MARS: Improving Visual Representations by Circumventing Text Feature Learning* <br> [Slides](pdf/t-mars.pdf) / [Paper](https://arxiv.org/abs/2307.03132)
- *What Do We Learn from Inverting CLIP Models?* <br> [Slides](pdf/clip_inversion.pdf) / [Paper](https://arxiv.org/abs/2403.02580)
- *GLIGEN: Open-Set Grounded Text-to-Image Generation* <br> [Slides](pdf/gligen.pdf) / [Paper](https://arxiv.org/abs/2301.07093)
- *SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis* <br> [Slides](pdf/sdxl.pdf) / [Paper](https://openreview.net/forum?id=di52zR8xgf)
- *Tree of Thoughts: Deliberate Problem Solving with Large Language Models* <br> [Slides](pdf/tree_of_thoughts.pdf) / [Paper](https://arxiv.org/abs/2305.10601)
- *Visual Instruction Tuning* <br> [Slides](pdf/visual_instruction_tuning.pdf) / [Paper](https://openreview.net/forum?id=w0H2xGHlkw)
- *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models* <br> [Slides](pdf/chain_of_thought_prompting.pdf) / [Paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf)
- *LoRA: Low-Rank Adaptation of Large Language Models* <br> [Slides](pdf/lora.pdf) / [Paper](https://arxiv.org/abs/2106.09685)
- *Visual Prompting via Image Inpainting* <br> [Slides](pdf/visual_prompting.pdf) / [Paper](https://openreview.net/forum?id=o4uFFg9_TpV)
- *Trainig Data Attribution for Diffusion Models* <br> [Slides](pdf/data_attribution.pdf) / [Paper](https://arxiv.org/abs/2306.02174)
- *Optimizing DDPM Sampling with Shortcut Fine-Tuning* <br> [Slides](pdf/sft.pdf) / [Paper](https://arxiv.org/abs/2301.13362)
- *Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models* <br> [Slides](pdf/unsupervised_rep_learn_ddpm.pdf) / [Paper](https://openreview.net/pdf?id=IiCsx9KNVa0)
- *SELF-REFINE: Iterative Refinement with Self-Feedback* <br> [Slides](pdf/self_refine.pdf) / [Paper](https://arxiv.org/abs/2303.17651)
- *Generative Agents: Interactive Simulacra of Human Behavior* <br> [Slides](pdf/generative_agents.pdf) / [Paper](https://arxiv.org/abs/2304.03442)
- *Consistency Models* <br> [Slides](pdf/consistency_models.pdf) / [Paper](https://proceedings.mlr.press/v202/song23a/song23a.pdf)
- *Image as Set of Points* <br> [Slides](pdf/img_as_set_of_points.pdf) / [Paper](https://openreview.net/forum?id=awnvqZja69)
- *Traditional Classification Neural Networks are Good Generators: They are Competitive with DDPMs and GANs* <br> [Slides](pdf/classifier_img_gen.pdf) / [Paper](https://arxiv.org/abs/2211.14794)
- *Git Re-Basin: Merging Models modulo Permutation Symmetries* <br> [Slides](pdf/git_rebasin.pdf) / [Paper](https://openreview.net/forum?id=CQsmMYmlP5T)











